{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd6f57-5669-40cc-bfcd-a9a4230cd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a Random Forest Classifier for predicting the risk of heart disease requires the following steps:\n",
    "\n",
    "1. Import the necessary libraries.\n",
    "2. Load the dataset.\n",
    "3. Preprocess the data (handle missing values, encode categorical variables, etc.).\n",
    "4. Split the dataset into training and testing sets.\n",
    "5. Create and train the Random Forest Classifier.\n",
    "6. Evaluate the model on the testing set.\n",
    "7. Fine-tune the hyperparameters if needed.\n",
    "\n",
    "Since I don't have access to the specific dataset you mentioned, I'll provide a general outline of the steps involved in building the model:\n",
    "\n",
    "```python\n",
    "# Step 1: Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = pd.read_csv(\"heart_disease_dataset.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data (if needed)\n",
    "\n",
    "# Step 4: Split the dataset into features (X) and target (y)\n",
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Create and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model on the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Please note that you need to replace \"heart_disease_dataset.csv\" with the actual filename of your dataset. Also, make sure to preprocess the data according to the characteristics of your dataset, including handling missing values and encoding categorical variables if required.\n",
    "\n",
    "Remember to fine-tune the hyperparameters of the Random Forest Classifier (e.g., `n_estimators`, `max_depth`, etc.) based on cross-validation to optimize the model's performance on your specific dataset. Additionally, it's crucial to interpret the results, analyze feature importance, and ensure the model's generalizability before deploying it in a real-world scenario.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "\n",
    "\n",
    "ANS-1\n",
    "\n",
    "\n",
    "To preprocess the dataset, we'll perform the following steps:\n",
    "\n",
    "1. Handle Missing Values: We'll check for missing values in the dataset and handle them appropriately, either by imputing the missing values or removing the rows/columns with missing data.\n",
    "\n",
    "2. Encode Categorical Variables: If the dataset contains categorical variables, we'll encode them into numerical format so that the Random Forest Classifier can process them.\n",
    "\n",
    "3. Scale Numerical Features: If the numerical features have different scales, we'll scale them to ensure that they have a similar range, which can improve the performance of the Random Forest Classifier.\n",
    "\n",
    "Let's assume the dataset has already been loaded as \"data\". We'll now perform the preprocessing steps:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Step 1: Handle Missing Values\n",
    "# Check for missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# If there are missing values, impute or remove them as needed\n",
    "# For example, if using mean imputation:\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Step 2: Encode Categorical Variables\n",
    "# Assuming the categorical variable is in the 'sex' column\n",
    "# We'll use OneHotEncoder to encode 'sex' into binary columns\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_sex = encoder.fit_transform(data[['sex']])\n",
    "encoded_sex_df = pd.DataFrame(encoded_sex, columns=['sex_encoded'])\n",
    "data = pd.concat([data, encoded_sex_df], axis=1)\n",
    "data.drop('sex', axis=1, inplace=True)\n",
    "\n",
    "# Step 3: Scale Numerical Features\n",
    "# Assuming the numerical features are in columns 'age', 'resting_blood_pressure', etc.\n",
    "numerical_features = ['age', 'resting_blood_pressure', 'serum_cholesterol', 'maximum_heart_rate_achieved']\n",
    "\n",
    "# Create a column transformer to scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers=[('num', scaler, numerical_features)], remainder='passthrough')\n",
    "\n",
    "# Apply the preprocessor to scale the numerical features\n",
    "scaled_data = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Please make sure to adapt the code based on your specific dataset and column names. The preprocessing steps may vary depending on the characteristics of your data. Additionally, if your dataset already contains numerical features on the same scale and does not have any missing values, you may skip the corresponding preprocessing steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "\n",
    "\n",
    "ANS-2\n",
    "\n",
    "\n",
    "To split the dataset into a training set and a test set, we can use the `train_test_split` function from the `sklearn.model_selection` module. We'll pass the features (X) and the target variable (y) along with the test_size parameter to determine the size of the test set.\n",
    "\n",
    "Let's split the dataset into a training set (70%) and a test set (30%):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already preprocessed the dataset and obtained X and y\n",
    "\n",
    "# Split the dataset into a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and test sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "```\n",
    "\n",
    "The `test_size` parameter is set to 0.3, which means that 30% of the data will be used for testing, and the remaining 70% will be used for training the model. The `random_state` parameter is set to 42 for reproducibility, ensuring that the same split is obtained when running the code multiple times. Adjust the `test_size` value according to your preferences and the size of your dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "ANS-3\n",
    "\n",
    "\n",
    "\n",
    "To train a Random Forest Classifier on the training set with 100 trees and a maximum depth of 10 for each tree, we can use the `RandomForestClassifier` from the `sklearn.ensemble` module. We'll set the `n_estimators` and `max_depth` hyperparameters accordingly.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest Classifier with 100 trees and max depth of 10\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Here, we set the `n_estimators` hyperparameter to 100, which means the Random Forest will consist of 100 decision trees. We also set the `max_depth` hyperparameter to 10, which limits the maximum depth of each decision tree to 10 levels. Setting a maximum depth can help prevent overfitting and improve the generalization of the model.\n",
    "\n",
    "The `random_state` parameter is set to 42 for reproducibility, ensuring that the same random initializations are used during training. You can adjust the `n_estimators`, `max_depth`, and other hyperparameters based on your specific problem and dataset. Keep in mind that tuning these hyperparameters can affect the model's performance, and it is often a good practice to perform hyperparameter tuning using techniques like grid search or random search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "\n",
    "\n",
    "ANS-4\n",
    "\n",
    "\n",
    "\n",
    "To evaluate the performance of the Random Forest Classifier on the test set, we can use the following metrics: accuracy, precision, recall, and F1 score. We'll use the predictions made by the model on the test set and compare them to the true labels.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict the target labels on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "```\n",
    "\n",
    "Each of these metrics provides different insights into the performance of the classifier:\n",
    "\n",
    "- **Accuracy:** The proportion of correct predictions among all predictions made by the model. It is a general measure of how well the classifier is performing overall.\n",
    "\n",
    "- **Precision:** The proportion of true positive predictions (correctly predicted positive instances) among all positive predictions made by the model. It indicates how many of the predicted positive instances are actually positive.\n",
    "\n",
    "- **Recall:** Also known as sensitivity or true positive rate, it is the proportion of true positive predictions among all actual positive instances. It measures the ability of the model to correctly identify positive instances.\n",
    "\n",
    "- **F1 Score:** The harmonic mean of precision and recall. It provides a balance between precision and recall and is particularly useful when the class distribution is imbalanced.\n",
    "\n",
    "Remember that the interpretation of these metrics depends on the specific problem and the importance of false positives and false negatives in your application. For example, in medical diagnostics, recall (sensitivity) may be more critical than precision (specificity) to minimize false negatives and ensure that all positive cases are correctly identified, even if it leads to more false positives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "\n",
    "\n",
    "ANS-5\n",
    "\n",
    "To identify the top 5 most important features in predicting heart disease risk using the Random Forest Classifier, we can access the feature importances after training the model. Then, we'll visualize the feature importances using a bar chart.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "feature_importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "# Sort the features based on their importances in descending order\n",
    "sorted_feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_5_features = sorted_feature_importances_df.head(5)\n",
    "\n",
    "# Visualize the feature importances using a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_5_features['Feature'], top_5_features['Importance'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 5 Most Important Features for Heart Disease Prediction')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In this code, we first retrieve the feature importances from the trained Random Forest model. Then, we create a DataFrame containing the feature names and their corresponding importances. After sorting the features based on their importances in descending order, we select the top 5 most important features. Finally, we visualize the top 5 features using a bar chart to highlight their relative importance in predicting heart disease risk.\n",
    "\n",
    "The bar chart will show the importance of each feature, with higher bars indicating more significant contributions to the model's prediction. The top 5 features will be displayed on the x-axis, and their corresponding importances will be represented by the height of the bars on the y-axis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "ANS-6\n",
    "\n",
    "\n",
    "\n",
    "To tune the hyperparameters of the Random Forest Classifier using Grid Search and perform 5-fold cross-validation to evaluate the performance, we can use the `GridSearchCV` from the `sklearn.model_selection` module. Grid Search allows us to specify different values for the hyperparameters and exhaustively try all combinations to find the best set of hyperparameters.\n",
    "\n",
    "Here's how to perform hyperparameter tuning using Grid Search:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the performance of the best model on the test set\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "```\n",
    "\n",
    "In this code, we first define a dictionary `param_grid` that contains different values for the hyperparameters `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`. The `GridSearchCV` will perform 5-fold cross-validation for each combination of hyperparameters and evaluate the performance of the model.\n",
    "\n",
    "After the grid search, we obtain the best hyperparameters found by `GridSearchCV` and create the best model using those hyperparameters. Finally, we evaluate the performance of the best model on the test set and calculate the accuracy, precision, recall, and F1 score.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "\n",
    "\n",
    "ANS-7\n",
    "\n",
    "\n",
    "\n",
    "To report the best set of hyperparameters found by the Grid Search and the corresponding performance metrics, we can use the `best_params_` attribute of the `GridSearchCV` object. Additionally, we will compare the performance of the tuned model with the default model.\n",
    "\n",
    "Here's how to do it:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the performance of the best model on the test set\n",
    "y_pred_tuned = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the tuned model\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "# Print the evaluation metrics for the tuned model\n",
    "print(\"Tuned Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_tuned)\n",
    "print(\"Precision:\", precision_tuned)\n",
    "print(\"Recall:\", recall_tuned)\n",
    "print(\"F1 Score:\", f1_tuned)\n",
    "\n",
    "# Train the default Random Forest Classifier\n",
    "default_rf_classifier = RandomForestClassifier(random_state=42)\n",
    "default_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the default model on the test set\n",
    "y_pred_default = default_rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the default model\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "precision_default = precision_score(y_test, y_pred_default)\n",
    "recall_default = recall_score(y_test, y_pred_default)\n",
    "f1_default = f1_score(y_test, y_pred_default)\n",
    "\n",
    "# Print the evaluation metrics for the default model\n",
    "print(\"Default Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_default)\n",
    "print(\"Precision:\", precision_default)\n",
    "print(\"Recall:\", recall_default)\n",
    "print(\"F1 Score:\", f1_default)\n",
    "```\n",
    "\n",
    "In this code, we print the best hyperparameters found by the Grid Search using `grid_search.best_params_`. Then, we get the best model from the grid search and evaluate its performance on the test set, calculating accuracy, precision, recall, and F1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "  Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk.\n",
    "\n",
    "\n",
    "ANS-8\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
